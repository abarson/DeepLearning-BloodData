\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    }
%\usepackage[margin=1in]{geometry}
\title{CS395 Spring 2018---Final Project Report \\ \large Heart Rate Prediction with Deep Video Regression}
\author{
  Adam Barson \\ \small{\texttt{abarson@uvm.edu}}
  \and Daniel Berenberg \\ \small{\texttt{djberenb@uvm.edu}
  \thanks{in collaboration with Dr. Ryan McGinnis, UVM} 
 \thanks{under advisory of Dr. Safwan Wsah, UVM}}
}
\date{\today}
\onehalfspace
\begin{document}
\maketitle
\section[1]{Introduction}
\noindent Millions of Americans are afflicted with panic disorder [1], a psychiatric disorder in which debilitating fear and anxiety arise with no apparent cause [2]. There are several clinically available methods to treat panic disorder, many of which involve either medication or intensive psychotherapy [3]. Past research [4] in the biomedical field has shown that by simply showing a panic disorder victim their heart rate on the onset of or during a panic attack, their episode was significantly mitigated or weakened in intensity.

\noindent Allowing people afflicted with panic disorder to access vitals such as heart rate and respiratory rate could not only benefit the longer term management of the disorder, but mitigate the risks and side effects of a live panic attack.

\noindent In order to expose this treatment method to as many victims of panic disorder as possible, we explore an element of the solution by making use of the ubiquitous smartphone. There is promising evidence that suggests pulse is detectable by processing videos in smartphone cameras. By pressing a finger against a smartphone camera with the flashlight activated, we can obtain a highly resolved clip of the blood pulsating within the finger. This project aims to leverage deep learning techniques in order to predict a patient?s heart rate from such a video. 
\section[2]{Problem Definition}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection[2.1]{Task Definition}
\noindent Our objective is to produce a fast, accurate within a small squared error, model that, given a sequence of the above described frames $S$ of images $X_{i} \ldots X_{n}$, we output a value $h{r}$, the heart rate of the individual who submitted the sequence. 

\noindent This problem is specifically intriguing not only for its real world application but because the task is vastly more dependent on the the temporal axis than other neural video processing tasks. 

\noindent The dataset used by this project has been provided by Dr. Ryan McGinnis of the University of Vermont Biomedical Engineering department. Video data was collected by sampling $31$ separate subjects; each patient recorded two videos of their finger-- one at a resting heart rate, and one after an intense $60$ second workout. All of the raw videos gathered during the study are around 30 seconds long. For each sample $V$, the heart rate $h_{r}$ and respiratory rate $r{v}$ for each were also recorded and are provided along with the video data.

\subsection[2.2]{Algorithm Definitions and Related Work}
\noindent There are various algorithms that apply to deep neural video processing tasks; we explored three: 2D stacked convolutions with a subsequent LSTM layer or LRCN [5], Two stream networks that incorporate the use of optical flow [6], and 3D-Convolution [7]. Each of these models is designed to extract features from the temporal element of the sample in a unique way.

\noindent The \textbf{LRCN} uses stacked convolutional layers in order to extract hierarchical, semantically meaningful feature vectors which are then fed into an LSTM layer which uses these features to determine long term dependencies throughout the sample. 

\noindent A \textbf{two stream network} uses two forms of input: raw frame sequences and the corresponding sequence of optical flow images that describes the change between some pair of frames. Formally, optical flow is defined as the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and the scene [8]. Since the optical flow transformation embeds temporal data in a 2D image, the two stream CNN is entirely a convolutional neural network with some sequence of dense layers at the end. Our adaptation of the two stream CNN adopts the temporal stream of the data and leaves the spatial stream. As noted previously, each frame is inconsequential in determining the heart rate of the video; the temporal correlations and color variations over time are much more semantically meaningful.

\noindent Finally, our third perspective on video regression stems from the \textbf{3D convolutional network}, which considers a volume and convolves throughout it, learning hierarchical features as per 2D. We use a stripped down, version of the C3D  [9], a commonly known and ?general purpose? 3D convolutional network, in which we several layers of filters in order to allow it to fit on our single Tesla K80.

\noindent Each of these models is a network which has a single, linearly activated output node for regression. These algorithms were each tested, with the 3D CNN ultimately performing the best.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[3]{Experimental Evaluation}
\subsection[3.1] {Methodology}
\noindent Since our output is a single, real valued number, we use the mean squared error (MSE) and root mean squared error (RMSE) metrics in order to summarize the performance of the model at a given point.

\noindent Since our dataset consisted of about 16 videos altogether, we used various on-the-fly (runtime) and offline (hard disk) augmentation techniques. Through several iterations, we eventually settled on scaling the speed of all videos in our dataset by all decade factors $a \in [0.5, 2.0]$ and assigning each of those newly augmented videos a label of $h_{r}a$, where $h_{r}$ is the heart rate of the original video. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection[3.2] {Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection[3.2.1]{Preliminary Investigation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection[3.2.2]{Intermediary Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection[3.2.3]{Temporal Stream}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection[3.2.4]{3D CNN Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection[3.3]{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection[3.3] {Discussion}
Our results show a promising glimpse of possible optimal behavior when training on our modified 3D convolutional architecture using temporal stream data. We attribute this added performance to the fact that optical flow data not only embeds more information per frame, but it also simply has a richer, more spatially unique feature set than that of the raw blood data. This allows the model to not only use the nature of 3D convolutions to extract temporal data, but to benefit from the fact that optical flow data is more informative on the frame level.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[4]{Code and Datasets}
\noindent The code base consists of a library called we\_panic\_utils, which contains several subpackages and modules for various forms of augmentation, preprocessing scripts, and architecture descriptions, video editing, dataset manipulation and creation, and other general utilities. Our library can be found \href{https://github.com/danielberenberg/DeepLearning-BloodData}{here}. After obtaining the data and organizing it correctly, the data must be preprocessed by running the \texttt{generate\_dataset} in a bash or simialar unix terminal. After preprocessing the data, use the \texttt{run\_it} bash script to interact with our codebase.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section[5]{Conclusion}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Bibliography}
\begin{enumerate}
\item ``Facts \& Statistics.? Anxiety and Depression Association of America, ADAA, adaa.org/about-adaa/press-room/facts-statistics\#.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item ``Panic Disorder.'' Anxiety and Depression Association of America, ADAA, adaa.org/understanding-anxiety/panic-disorder\#.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item ``Panic Disorder: When Fear Overwhelms.'' National Institute of Mental Health, U.S. Department of Health and Human Services, \\ \href{www.nimh.nih.gov/health/publications/panic-disorder-when-fear-overwhelms/index.shtml}{link}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item  Ehlers, Anke, and Peter Breuer. ``How Good Are Patients with Panic Disorder at Perceiving Their Heartbeats?'' Biological Psychology 42, no. 1-2 (1996): 165-82. doi:10.1016/0301-0511(95)05153-8.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Donahue, Jeff, et al. ``Long-Term Recurrent Convolutional Networks for Visual Recognition and Description.'' 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, doi:10.1109/cvpr.2015.7298878.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item  Zhao, Rui, et al. ``Two-Stream RNN/CNN for Action Recognition in 3D Videos.'' 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2017, doi:10.1109/iros.2017.8206288.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Li, Chao, et al. ``End-to-End Learning of Deep Convolutional Neural Network for 3D Human Action Recognition.'' 2017 IEEE International Conference on Multimedia \& Expo Workshops (ICMEW), 2017, doi:10.1109/icmew.2017.8026281.
\item Kelson R. T. Aires; Andre M. Santana; Adelardo A. D. Medeiros (2008). Optical Flow Using Color Information. ACM New York, NY, USA. ISBN 978-1-59593-753-7.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, Learning Spatiotemporal Features with 3D Convolutional Networks, ICCV 2015,
\end{enumerate}
\end{document}